{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece datasets translate-toolkit --quiet"
      ],
      "metadata": {
        "id": "uA3VM0N-bfqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "from IPython.utils import io\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "with io.capture_output() as captured:\n",
        "  !pip install transformers sentencepiece\n",
        "\n",
        "from transformers import AdamW, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers.models.mt5 import MT5Config, MT5ForConditionalGeneration\n"
      ],
      "metadata": {
        "id": "saHDlGyTbi29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-Tatoeba/v2021-07-22/tmx/en-ru.tmx.gz"
      ],
      "metadata": {
        "id": "1ySxtCZeboB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d /content/en-ru.tmx.gz"
      ],
      "metadata": {
        "id": "GMy_LEvebpqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"en-ru.tmx\", 'r', encoding=\"utf-8\") as input_file:\n",
        "  for x in range(50):\n",
        "    print(input_file.readline())"
      ],
      "metadata": {
        "id": "5p8QaNrObsw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from translate.storage.tmx import tmxfile\n",
        "\n",
        "with open(\"en-ru.tmx\", 'rb') as input_file:\n",
        "  tmx_file = tmxfile(input_file, 'en', 'ru')"
      ],
      "metadata": {
        "id": "mSxcfRe5b1Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "for node in tmx_file.unit_iter():\n",
        "  dataset.append({'en': node.source, 'ru': node.target})"
      ],
      "metadata": {
        "id": "S7vq9G3gb3-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset[:10000]\n",
        "test_dataset = dataset[10000:15000]"
      ],
      "metadata": {
        "id": "y2ZgAwIRb6C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "k = random.randint(0, 100000)\n",
        "\n",
        "print(k)"
      ],
      "metadata": {
        "id": "0mj8vF4mb6Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_repo = 'google/mt5-base'\n",
        "\n",
        "config = MT5Config.from_pretrained(model_repo)"
      ],
      "metadata": {
        "id": "amntccv-b_EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_repo)"
      ],
      "metadata": {
        "id": "eIMWW9D3cBbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MT5ForConditionalGeneration.from_pretrained(model_repo)"
      ],
      "metadata": {
        "id": "BdM7iIrZcCxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANG_TOKEN_MAPPING = {\n",
        "    'ru': '<ru>',\n",
        "    'en': '<en>'\n",
        "}"
      ],
      "metadata": {
        "id": "eglAcKCycFA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_str = '<ru>Привет Мир!.'\n",
        "input_ids = tokenizer.encode(example_input_str,\n",
        "                             return_tensors='pt',\n",
        "                             padding='max_length',\n",
        "                             truncation=True,\n",
        "                             max_length=40)\n",
        "print(input_ids)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "XYEFTG5GcFnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
        "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
        "  target_lang_token = lang_token_map[target_lang]\n",
        "\n",
        "  # Tokenize and add special tokens\n",
        "  input_ids = tokenizer.encode(\n",
        "      text = target_lang_token + text,\n",
        "      return_tensors = 'pt',\n",
        "      padding = 'max_length',\n",
        "      truncation = True,\n",
        "      max_length = seq_len)\n",
        "\n",
        "  return input_ids[0]\n",
        "\n",
        "def encode_target_str(text, tokenizer, seq_len):\n",
        "  token_ids = tokenizer.encode(\n",
        "      text = text,\n",
        "      return_tensors = 'pt',\n",
        "      padding = 'max_length',\n",
        "      truncation = True,\n",
        "      max_length = seq_len)\n",
        "\n",
        "  return token_ids[0]\n",
        "\n",
        "def format_translation_data(translations, lang_token_map,\n",
        "                            tokenizer, seq_len=128):\n",
        "  # Choose a random 2 languages for in i/o\n",
        "  langs = list(lang_token_map.keys())\n",
        "  input_lang, target_lang = np.random.choice(langs, size=2, replace=False)\n",
        "\n",
        "  # Get the translations for the batch\n",
        "  input_text = translations[input_lang]\n",
        "  target_text = translations[target_lang]\n",
        "\n",
        "  # print(input_lang, input_text)\n",
        "  # print(target_lang, target_text)\n",
        "\n",
        "  if input_text is None or target_text is None:\n",
        "    return None\n",
        "\n",
        "  input_token_ids = encode_input_str(input_text, target_lang, tokenizer, seq_len, lang_token_map)\n",
        "  target_token_ids = encode_target_str(target_text, tokenizer, seq_len)\n",
        "\n",
        "  return input_token_ids, target_token_ids\n",
        "\n",
        "def transform_batch(batch, lang_token_map, tokenizer):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for translation_set in batch:\n",
        "    formatted_data = format_translation_data(translation_set, lang_token_map, tokenizer, max_seq_len)\n",
        "\n",
        "    # print(formatted_data)\n",
        "\n",
        "    if formatted_data is None:\n",
        "      continue\n",
        "\n",
        "    input_ids, target_ids = formatted_data\n",
        "    inputs.append(input_ids.unsqueeze(0))\n",
        "    targets.append(target_ids.unsqueeze(0))\n",
        "\n",
        "  batch_input_ids = torch.cat(inputs).cuda()\n",
        "  batch_target_ids = torch.cat(targets).cuda()\n",
        "\n",
        "  return batch_input_ids, batch_target_ids\n",
        "\n",
        "def get_data(dataset, lang_token_map, tokenizer, batch_size=32):\n",
        "  np.random.shuffle(dataset)\n",
        "  for i in range(0, len(dataset), batch_size):\n",
        "    raw_batch = dataset[i:i+batch_size]\n",
        "\n",
        "    yield transform_batch(raw_batch, lang_token_map, tokenizer)"
      ],
      "metadata": {
        "id": "UgS-5g5NcK-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5\n",
        "batch_size = 15\n",
        "print_freq = 100\n",
        "max_seq_len=40\n",
        "lr = 5e-4\n",
        "\n",
        "checkpoint_freq = 1000\n",
        "\n",
        "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
        "total_steps = n_epochs * n_batches\n",
        "n_warmup_steps = int(total_steps * 0.01)\n",
        "\n",
        "print(\"n_batches\", n_batches)\n",
        "print(\"total_steps\", total_steps)\n",
        "print(\"n_warmup_steps\", n_warmup_steps)"
      ],
      "metadata": {
        "id": "B0SL2ajwcMcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, n_warmup_steps, total_steps)\n",
        "\n",
        "losses = []\n",
        "test_losses = []"
      ],
      "metadata": {
        "id": "ZvbKWAr9cSXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, dataset, max_iters=8):\n",
        "  test_generator = get_data(dataset, LANG_TOKEN_MAPPING,\n",
        "                                      tokenizer, batch_size)\n",
        "  eval_losses = []\n",
        "  with torch.no_grad():\n",
        "    for i, (input_batch, label_batch) in enumerate(test_generator):\n",
        "      if i >= max_iters:\n",
        "        break\n",
        "\n",
        "      model_out = model.forward(\n",
        "          input_ids = input_batch,\n",
        "          labels = label_batch)\n",
        "      eval_losses.append(model_out.loss.item())\n",
        "\n",
        "  return np.mean(eval_losses)"
      ],
      "metadata": {
        "id": "sthxoo-_cmdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_test_loss = float('inf')"
      ],
      "metadata": {
        "id": "Y0sriLkjcn-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_idx in range(n_epochs):\n",
        "  data_generator = get_data(train_dataset, LANG_TOKEN_MAPPING, tokenizer, batch_size)\n",
        "\n",
        "  for batch_idx, (input_batch, label_batch) in tqdm(enumerate(data_generator), total=n_batches):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    model_out = model.forward(\n",
        "        input_ids = input_batch,\n",
        "        labels = label_batch)\n",
        "\n",
        "    loss = model_out.loss\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "      # Print training update info\n",
        "    if (batch_idx + 1) % print_freq == 0:\n",
        "      avg_loss = np.mean(losses[-print_freq:])\n",
        "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {:.6f}'.format(\n",
        "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
        "\n",
        "    if (batch_idx + 1) % checkpoint_freq == 0:\n",
        "      test_loss = eval_model(model, test_dataset)\n",
        "      test_losses.append(test_loss)\n",
        "      print('Test loss {:.3f}'.format(test_loss))\n",
        "      if best_test_loss > test_loss:\n",
        "        print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        best_test_loss = test_loss\n",
        "\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "dH1AdKfZcp4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 50\n",
        "smoothed_losses = []\n",
        "for i in range(len(losses)-window_size):\n",
        "  smoothed_losses.append(np.mean(losses[i:i+window_size]))\n",
        "\n",
        "plt.plot(smoothed_losses[100:])"
      ],
      "metadata": {
        "id": "YnHlsqDQc09O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_losses[:])"
      ],
      "metadata": {
        "id": "EHtZYSeVc2oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = test_dataset[16]['en']\n",
        "print('Raw input text:', test_sentence)\n",
        "\n",
        "input_ids = encode_input_str(\n",
        "    text = test_sentence,\n",
        "    target_lang = 'ru',\n",
        "    tokenizer = tokenizer,\n",
        "    seq_len = model.config.max_length,\n",
        "    lang_token_map = LANG_TOKEN_MAPPING)\n",
        "\n",
        "input_ids = input_ids.unsqueeze(0).cuda()\n",
        "\n",
        "print('Truncated input text:', tokenizer.convert_tokens_to_string(\n",
        "    tokenizer.convert_ids_to_tokens(input_ids[0])))"
      ],
      "metadata": {
        "id": "RLvZw_bLc5Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=3)\n",
        "\n",
        "for token_set in output_tokens:\n",
        "  print(tokenizer.decode(token_set, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ofh5u4ofdOeN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}